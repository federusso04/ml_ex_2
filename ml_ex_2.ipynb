{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox_VKJ2nA5Ai"
      },
      "source": [
        "# **Esercitazione 2 - Regressione Lineare**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZhLuEm6A5Ai"
      },
      "source": [
        "## Boston Housing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yBnsPI_A5Ai"
      },
      "source": [
        "Questo dataset contiene informazioni raccolte dal U.S. Census Service riguardanti le abitazioni nell'area di Boston, Massachusetts. È stato ottenuto dall'archivio StatLib (http://lib.stat.cmu.edu/datasets/boston) ed è stato ampiamente utilizzato in letteratura per fare benchmark di algoritmi.\n",
        "\n",
        "Il dataset contiene informazioni su 506 case, divise in 14 variabili."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:02.226428Z",
          "iopub.status.busy": "2025-03-31T10:24:02.226039Z",
          "iopub.status.idle": "2025-03-31T10:24:02.230853Z",
          "shell.execute_reply": "2025-03-31T10:24:02.229758Z",
          "shell.execute_reply.started": "2025-03-31T10:24:02.226384Z"
        },
        "trusted": true,
        "id": "CkyATYtsA5Aj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:02.241952Z",
          "iopub.status.busy": "2025-03-31T10:24:02.241456Z",
          "iopub.status.idle": "2025-03-31T10:24:07.646609Z",
          "shell.execute_reply": "2025-03-31T10:24:07.645511Z",
          "shell.execute_reply.started": "2025-03-31T10:24:02.241906Z"
        },
        "trusted": true,
        "id": "dTmy2z_uA5Aj",
        "outputId": "0643aac0-8ede-4000-aa1b-188c50f0cf66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (506, 13), targets shape:  (506,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from pandas import read_csv\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "\n",
        "# Scarica il Boston Housing Dataset da OpenML\n",
        "boston = fetch_openml(name=\"Boston\", version=1, as_frame=True)\n",
        "\n",
        "# Estrai i dati (features) e il target (valore mediano delle abitazioni)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "X, y = shuffle(X, y, random_state=0)\n",
        "print(f\"Features shape: {X.shape}, targets shape:  {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4OoD5qjA5Ak"
      },
      "source": [
        "## `np.c_` in NumPy\n",
        "\n",
        "L'oggetto `np.c_` in NumPy è una **scorciatoia** per concatenare array lungo il secondo asse (cioè, le colonne).\n",
        "\n",
        "## Utilizzo\n",
        "```python\n",
        "np.c_[array1, array2, ...]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:07.648437Z",
          "iopub.status.busy": "2025-03-31T10:24:07.647916Z",
          "iopub.status.idle": "2025-03-31T10:24:07.655823Z",
          "shell.execute_reply": "2025-03-31T10:24:07.654516Z",
          "shell.execute_reply.started": "2025-03-31T10:24:07.648409Z"
        },
        "trusted": true,
        "id": "o5rU8qHeA5Ak",
        "outputId": "ab746067-d290-42f1-b559-2a8717eed7f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrice 1: (2, 3)\n",
            "\n",
            "Matrice 2: (2, 3)\n",
            "\n",
            "Matrice concatenata: (2, 6)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate two random 2x3 matrices\n",
        "matrice1 = np.random.rand(2, 3)\n",
        "matrice2 = np.random.rand(2, 3)\n",
        "\n",
        "# Concatenate the matrices along columns\n",
        "risultato = np.c_[matrice1, matrice2]\n",
        "\n",
        "print(\"Matrice 1:\",matrice1.shape)\n",
        "\n",
        "print(\"\\nMatrice 2:\",matrice2.shape)\n",
        "\n",
        "print(\"\\nMatrice concatenata:\",risultato.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j-0r6PeA5Ak"
      },
      "source": [
        "**Divisione del dataset**\n",
        "\n",
        "Il primo passaggio è quello di dividere i dati in train set, validation set e test set. Utilizza il 60% dei dati per il training set, il 20% per il validation e il restante 20% per il test set. Considerato che il nostro dataset possiede 506 osservazioni mi aspetto che:\n",
        "\n",
        "- Il **training set** avrà 303 osservazioni.\n",
        "- Il **validation set** avrà 101 osservazioni.\n",
        "- Il **test set** avrà 101 osservazioni.\n",
        "\n",
        "In reatà il test set avrà 102 osservazioni per via delle approssimazioni.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:07.658414Z",
          "iopub.status.busy": "2025-03-31T10:24:07.658110Z",
          "iopub.status.idle": "2025-03-31T10:24:07.675490Z",
          "shell.execute_reply": "2025-03-31T10:24:07.674427Z",
          "shell.execute_reply.started": "2025-03-31T10:24:07.658388Z"
        },
        "trusted": true,
        "id": "rMEcr6E4A5Ak"
      },
      "outputs": [],
      "source": [
        "# Divisione del dataset\n",
        "\n",
        "train_porzione = 0.6\n",
        "val_porzione = 0.2\n",
        "test_porzione = 0.2\n",
        "\n",
        "# svolgimento...\n",
        "n = X.shape[0]\n",
        "n_train = int(n * train_porzione)\n",
        "n_val = int(n * val_porzione)\n",
        "n_test = n - n_train - n_val  # per gestire eventuali arrotondamenti\n",
        "\n",
        "X_train = X[:n_train]\n",
        "y_train = y[:n_train]\n",
        "\n",
        "X_val = X[n_train:n_train+n_val]\n",
        "y_val = y[n_train:n_train+n_val]\n",
        "\n",
        "X_test = X[n_train+n_val:]\n",
        "y_test = y[n_train+n_val:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIJb2AhzA5Ak"
      },
      "source": [
        "### **Esercizio 1: Costruisci una Pipeline di Regressione Lineare Standardizzata**\n",
        "\n",
        "**Step 1:** Standardizza i dataset di addestramento, validazione e test. Usa `StandardScaler` di scikit-learn.  \n",
        "\n",
        "**Step 2:** Aggiungi una feature costante (bias) ai dati concatenando una colonna di uno ad ogni dataset.  \n",
        "\n",
        "**Step 3:** Implementa la soluzione in forma chiusa per l'addestramento di un modello di regressione lineare.\n",
        "\n",
        "**Step 4:** Valuta il modello calcolando il Mean Absolute Error (MAE) sui dataset di addestramento, validazione e test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARK_1uA3A5Ak"
      },
      "source": [
        "### **Guida**\n",
        "\n",
        "1. **StandardScaler**:\n",
        "   - Utilizza `StandardScaler` da `sklearn.preprocessing` per standardizzare i dati.\n",
        "   - Il metodo `fit_transform` calcola la media e la varianza dei dati di addestramento e li scala di conseguenza.\n",
        "   - Utilizza `transform` per standardizzare i dati di validazione e test utilizzando gli stessi parametri. Utilizziamo il metodo `transform` perchè non calcola i parametri di scaling (media e std). In questo modo ci assicuriamo che i dati di training e quelli di validation e test vengano scalati in modo uguale. Se usassimo `fit_transform` avremmo degli scaling diversi.\n",
        "\n",
        "2. **Aggiunta di una Caratteristica Costante**:\n",
        "   - Utilizza `np.c_` per concatenare una colonna di uno alle matrici delle caratteristiche. Questo è importante per includere il termine di intercetta nella regressione lineare.\n",
        "\n",
        "3. **Soluzione in Forma Chiusa per la Regressione Lineare**:\n",
        "   - La soluzione in forma chiusa è:\n",
        "\n",
        "     $$\\theta = (X^T X)^{-1} X^T y$$\n",
        "\n",
        "   - Per calcolare la trasposta di una matrice possiamo utilizzare l' attributo `.T` di cui ogni array è dotato.\n",
        "\n",
        "   - Utilizza `np.linalg.inv` di NumPy per l'inversione della matrice e l'operatore `@` per la moltiplicazione matriciale.\n",
        "  \n",
        "   - Puoi utilizzare l'operatore @ per eseguire l'operazione np.dot (`A @ B` è equivalente a `np.dot(A, B)`).\n",
        "\n",
        "4. **Mean Absolute Error (MAE)**:\n",
        "   - L'MAE si calcola come:\n",
        "\n",
        "     $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n",
        "\n",
        "   - Utilizza `np.mean` e `np.abs` per calcolarlo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:07.677620Z",
          "iopub.status.busy": "2025-03-31T10:24:07.677209Z",
          "iopub.status.idle": "2025-03-31T10:24:07.695610Z",
          "shell.execute_reply": "2025-03-31T10:24:07.694648Z",
          "shell.execute_reply.started": "2025-03-31T10:24:07.677583Z"
        },
        "trusted": true,
        "id": "kbQcTYFgA5Ak"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Normalizzazione dei dati. Dobbiamo normalizzare le features\n",
        "# sia del training set, validation set e test set.\n",
        "\n",
        "# Utilizziamo il metodo .fit_transform() dello scaler per normalizzare le feature di training.\n",
        "\n",
        "# Per normalizzare le feature di validation e test utilizziamo il metodo .transform()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# svolgimento...\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:07.697003Z",
          "iopub.status.busy": "2025-03-31T10:24:07.696682Z",
          "iopub.status.idle": "2025-03-31T10:24:07.713127Z",
          "shell.execute_reply": "2025-03-31T10:24:07.711755Z",
          "shell.execute_reply.started": "2025-03-31T10:24:07.696978Z"
        },
        "trusted": true,
        "id": "EGyNFTRPA5Ak"
      },
      "outputs": [],
      "source": [
        "# Step 2 - Aggiunta di una feature costante\n",
        "\n",
        "# creiamo un vettore di 1 da aggiungere come feature costante.\n",
        "# ATTENZIONE: questo vettore deve avere le stesse righe del set a cui viene aggiunto.\n",
        "# Uno uguale per tutti non va bene\n",
        "\n",
        "# svolgimento...\n",
        "X_train_const = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
        "X_val_const = np.c_[np.ones((X_val_scaled.shape[0], 1)), X_val_scaled]\n",
        "X_test_const = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:07.714546Z",
          "iopub.status.busy": "2025-03-31T10:24:07.714182Z",
          "iopub.status.idle": "2025-03-31T10:24:07.730379Z",
          "shell.execute_reply": "2025-03-31T10:24:07.729386Z",
          "shell.execute_reply.started": "2025-03-31T10:24:07.714505Z"
        },
        "trusted": true,
        "id": "R_PQ_rASA5Al"
      },
      "outputs": [],
      "source": [
        "# Step 3 - Applichiamo la formula matematica della regressione lineare\n",
        "\n",
        "# ATTENZIONE: stiamo per effettuare operazioni tra matrici e vettori,\n",
        "# non si tratta di una semplice formula matematica, stiamo attenti a quali operatori utilizzare e quanto\n",
        "\n",
        "# svolgimento...\n",
        "theta = np.linalg.inv(X_train_const.T @ X_train_const) @ (X_train_const.T @ y_train)\n",
        "\n",
        "y_train_pred = X_train_const @ theta\n",
        "y_val_pred   = X_val_const @ theta\n",
        "y_test_pred  = X_test_const @ theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:07.732004Z",
          "iopub.status.busy": "2025-03-31T10:24:07.731608Z",
          "iopub.status.idle": "2025-03-31T10:24:07.749354Z",
          "shell.execute_reply": "2025-03-31T10:24:07.747903Z",
          "shell.execute_reply.started": "2025-03-31T10:24:07.731963Z"
        },
        "trusted": true,
        "id": "2W7qCge4A5Al"
      },
      "outputs": [],
      "source": [
        "# Step 4 - Calcolo MAE\n",
        "\n",
        "# Calcoliamo l'errore medio assoluto (MAE) per il training set, validation set e test set.\n",
        "# Utlizziamo la formula specificata nella guida.\n",
        "\n",
        "# svolgimento...\n",
        "mae_train = np.mean(np.abs(y_train - y_train_pred))\n",
        "mae_val   = np.mean(np.abs(y_val - y_val_pred))\n",
        "mae_test  = np.mean(np.abs(y_test - y_test_pred))\n",
        "\n",
        "print(\"MAE (soluzione chiusa):\")\n",
        "print(\"Training:\", mae_train)\n",
        "print(\"Validation:\", mae_val)\n",
        "print(\"Test:\", mae_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IKG0pZQA5Al"
      },
      "source": [
        "### **Esercizio: Costruisci una pipeline di Regressione Lineare Standardizzata utilizzando `scikit-learn`**\n",
        "\n",
        "**Step 1 & 2:** Step 1 e 2 sono uguali a quanto fatto prima.\n",
        "\n",
        "**Step 3:** Utilizza `LinearRegression()` di scikit-learn per addestrare un modello di regressione lineare.  \n",
        "\n",
        "**Step 4:** Valuta il modello calcolando il Mean Absolute Error (MAE) sui dataset di addestramento, validazione e test, utilizzando `mean_absolute_error()` da `sklearn.metrics`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wsPON3bA5Al"
      },
      "source": [
        "## `LinearRegression` da Scikit-Learn\n",
        "\n",
        "La classe `LinearRegression` in Scikit-Learn viene utilizzata per eseguire la **regressione lineare**, adattando un modello lineare al dataset.\n",
        "\n",
        "## **Sintassi**\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "# Dati di esempio\n",
        "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
        "y = np.array([10, 15, 20, 25])\n",
        "\n",
        "# Adatta il modello ai dati\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predici nuovi valori\n",
        "X_new = np.array([[3, 5], [5, 9]])\n",
        "predictions = model.predict(X_new)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4LBB0FxA5Al"
      },
      "source": [
        "## `mean_absolute_error` da Scikit-Learn\n",
        "\n",
        "La funzione `mean_absolute_error` calcola l'**errore assoluto medio** (MAE) tra i valori target reali e quelli predetti.\n",
        "\n",
        "## **Sintassi**\n",
        "```python\n",
        "sklearn.metrics.mean_absolute_error(y_true, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1fIxQU3A5Al"
      },
      "source": [
        "### **Guida**\n",
        "\n",
        "1. **Istanziare e allenare un modello di regressione lineare**:\n",
        "    \n",
        "    - Istanziamo una classe `LinearRegression` per creare il modello.\n",
        "    - Utilizziamo il metodo `.fit()` per allenare il modello con i dati di training.\n",
        "\n",
        "2. **Effettuare predizioni con il modello**:\n",
        "\n",
        "    - Utiliziamo il metodo `.predict()` del modello per effettuare le predizioni. Effettuiamo le predizioni per tutti i set che abbiamo (train, validation e test).\n",
        "\n",
        "3. **Calcolo della MAE**:\n",
        "\n",
        "    - Calcolare MAE su tutti i set utilizzando la funzione `mean_abslute_error`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:07.750953Z",
          "iopub.status.busy": "2025-03-31T10:24:07.750627Z",
          "iopub.status.idle": "2025-03-31T10:24:07.993529Z",
          "shell.execute_reply": "2025-03-31T10:24:07.992400Z",
          "shell.execute_reply.started": "2025-03-31T10:24:07.750917Z"
        },
        "trusted": true,
        "id": "62lJhFmJA5Al"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:07.996378Z",
          "iopub.status.busy": "2025-03-31T10:24:07.996050Z",
          "iopub.status.idle": "2025-03-31T10:24:08.000523Z",
          "shell.execute_reply": "2025-03-31T10:24:07.999265Z",
          "shell.execute_reply.started": "2025-03-31T10:24:07.996338Z"
        },
        "trusted": true,
        "id": "QdagythHA5Al"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Istanziare e allenare il modello di regressione lineare.\n",
        "\n",
        "# svolgimento...\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_const, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:08.002488Z",
          "iopub.status.busy": "2025-03-31T10:24:08.001888Z",
          "iopub.status.idle": "2025-03-31T10:24:08.018572Z",
          "shell.execute_reply": "2025-03-31T10:24:08.017603Z",
          "shell.execute_reply.started": "2025-03-31T10:24:08.002449Z"
        },
        "trusted": true,
        "id": "JuTh4oz6A5Al"
      },
      "outputs": [],
      "source": [
        "# Step 2 - Effettuare predizioni\n",
        "\n",
        "# svolgimento...\n",
        "y_train_pred_lr = model.predict(X_train_const)\n",
        "y_val_pred_lr   = model.predict(X_val_const)\n",
        "y_test_pred_lr  = model.predict(X_test_const)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:08.020090Z",
          "iopub.status.busy": "2025-03-31T10:24:08.019699Z",
          "iopub.status.idle": "2025-03-31T10:24:08.037187Z",
          "shell.execute_reply": "2025-03-31T10:24:08.036031Z",
          "shell.execute_reply.started": "2025-03-31T10:24:08.020051Z"
        },
        "trusted": true,
        "id": "Z3yZnzu3A5Al"
      },
      "outputs": [],
      "source": [
        "# Step 3 - Calcolo MAE\n",
        "\n",
        "# svolgimento...\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mae_train_lr = mean_absolute_error(y_train, y_train_pred_lr)\n",
        "mae_val_lr   = mean_absolute_error(y_val, y_val_pred_lr)\n",
        "mae_test_lr  = mean_absolute_error(y_test, y_test_pred_lr)\n",
        "\n",
        "print(\"\\nMAE (LinearRegression di scikit-learn):\")\n",
        "print(\"Training:\", mae_train_lr)\n",
        "print(\"Validation:\", mae_val_lr)\n",
        "print(\"Test:\", mae_test_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aeDRTDkA5Al"
      },
      "source": [
        "### **Esercizio: Crea una funzione che esegua una pipeline di Regressione Lineare**\n",
        "\n",
        "La funzione deve richiedere un parametro `hyperparams` per gestire i diversi casi.\n",
        "\n",
        "`hyperparams` deve essere un dizionario contenente diverse chiavi, in base al valore di queste chiavi devono essere eseguiti (oppure no) diversi pezzi di codice.\n",
        "\n",
        "In questo esercizio la chiave da utilizzare sarà `hyperparams['data_standardize']`. Se il valore di questa chiave sarà **True** allora eseguire la standardizzazione con `scikit-learn`, se invece è **False** non verrà eseguita alcuna standardizzazione.\n",
        "\n",
        "**Step 1:** Controllare se eseguire o no la standardizzazione.\n",
        "\n",
        "* **Step 1.1:** Scrivere il codice per eseguire la standardizzazione.\n",
        "\n",
        "**Step 2:** Utilizza `np.c_` per concatenare una colonna di uno alle matrici delle caratteristiche.\n",
        "\n",
        "**Step 3:** Applichiamo la formula matematica della regressione lineare.\n",
        "\n",
        "**Step 4:** Calcolo MAE utilizzando la formula (NON con `scikit-learn`).\n",
        "\n",
        "La funzione deve ritornare i valori della MAE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmpSTY_1A5Al"
      },
      "source": [
        "Dopo aver testato i risultati con `hyperparams['data_standardize']` = **True**, provare anche i risultati ottenuti se `hyperparams['data_standardize']` = **False**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-31T10:24:08.038883Z",
          "iopub.status.busy": "2025-03-31T10:24:08.038467Z",
          "iopub.status.idle": "2025-03-31T10:24:08.058693Z",
          "shell.execute_reply": "2025-03-31T10:24:08.057041Z",
          "shell.execute_reply.started": "2025-03-31T10:24:08.038844Z"
        },
        "trusted": true,
        "id": "pRMVrFnDA5Al",
        "outputId": "05570df8-9695-4ee4-966b-527de2f236ae"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-13-bd02c9e519cb>, line 20)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-bd02c9e519cb>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    # Step 4 - Calcolare MAE\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "# svolgimento...\n",
        "\n",
        "def pipeline(X_train, y_train, X_val, y_val, hyperparams):\n",
        "\n",
        "    X_train = np.array(X_train, dtype=float)\n",
        "    y_train = np.array(y_train, dtype=float)\n",
        "    X_val = np.array(X_val, dtype=float)\n",
        "    y_val = np.array(y_val, dtype=float)\n",
        "\n",
        "    # Step 1 - Controllo se è richiesta la standardizzazione dei dati\n",
        "    if hyperparams.get('use_pca', False):\n",
        "        pca = PCA(n_components=hyperparams.get('pca_components', None))\n",
        "        X_train = pca.fit_transform(X_train)\n",
        "        X_val   = pca.transform(X_val)\n",
        "\n",
        "        # Step 1.1 - Scrivere il codice per standardizzare i dati\n",
        "\n",
        "\n",
        "    # Step 2 - Concatenare una colonna di uno alla matrice delle features\n",
        "    if hyperparams.get('data_standardize', False):\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_val   = scaler.transform(X_val)\n",
        "\n",
        "    # Step 3 - Applicare formula della regressione lineare e calcolare predizioni\n",
        "    X_train_const = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "    X_val_const   = np.c_[np.ones((X_val.shape[0], 1)), X_val]\n",
        "\n",
        "    # Step 4 - Calcolare MAE\n",
        "    mae_train = np.mean(np.abs(y_train - y_train_pred))\n",
        "    mae_val   = np.mean(np.abs(y_val - y_val_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-31T10:24:08.059232Z",
          "iopub.status.idle": "2025-03-31T10:24:08.059592Z",
          "shell.execute_reply": "2025-03-31T10:24:08.059458Z"
        },
        "trusted": true,
        "id": "4fF9PvYSA5Am"
      },
      "outputs": [],
      "source": [
        "hyperparams = {'data_standardize': True}\n",
        "\n",
        "train_fraction = 0.8\n",
        "validation_fraction = 0.2\n",
        "\n",
        "num_train = int(train_fraction * X.shape[0])\n",
        "\n",
        "X_train = X[:num_train]\n",
        "y_train = y[:num_train]\n",
        "\n",
        "X_validation = X[num_train:]\n",
        "y_validation = y[num_train:]\n",
        "\n",
        "# Chiamare la funzione pipeline e stampare i risultati della MAE\n",
        "\n",
        "# svolgimento..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3yucIkqA5Am"
      },
      "source": [
        "### **Esercizio: Implementare alla funzione `pipeline` la possibilità di usare PCA**\n",
        "\n",
        "Modifichiamo la funzione `pipeline` in modo da gestire anche la possibilità di effettuare la PCA. Dunque aggiungiamo al dizionario `hyperparams` la chiave `use_pca`.\n",
        "\n",
        "Se `hyperparams['use_pca']` = **True** verrà eseguita la PCA.\n",
        "\n",
        "Se `hyperparams['use_pca']` = **False** non verrà eseguita la PCA.\n",
        "\n",
        "La gestione della standardizzazione deve essere mantenuta come prima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-31T10:24:08.060507Z",
          "iopub.status.idle": "2025-03-31T10:24:08.061121Z",
          "shell.execute_reply": "2025-03-31T10:24:08.060904Z"
        },
        "trusted": true,
        "id": "7WAGypHeA5Am"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def pipeline(X_train, y_train, X_val, y_val, hyperparams):\n",
        "\n",
        "    # Conversione in array di tipo float\n",
        "    X_train = np.array(X_train, dtype=float)\n",
        "    y_train = np.array(y_train, dtype=float)\n",
        "    X_val   = np.array(X_val, dtype=float)\n",
        "    y_val   = np.array(y_val, dtype=float)\n",
        "\n",
        "    # Step 1 - Controllo se è richiesta la PCA\n",
        "    if hyperparams.get('use_pca', False):\n",
        "        # Se presente, recuperiamo il numero di componenti desiderato, altrimenti usiamo tutte le componenti\n",
        "        n_components = hyperparams.get('pca_components', None)\n",
        "        pca = PCA(n_components=n_components)\n",
        "        # Adattiamo la PCA sui dati di training e trasformiamo anche i dati di validation\n",
        "        X_train = pca.fit_transform(X_train)\n",
        "        X_val   = pca.transform(X_val)\n",
        "\n",
        "    # Step 2 - Controllo se è richiesta la standardizzazione dei dati\n",
        "    if hyperparams.get('data_standardize', False):\n",
        "        scaler = StandardScaler()\n",
        "        # Standardizziamo i dati: fit sui dati di training e trasformiamo anche i dati di validation\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_val   = scaler.transform(X_val)\n",
        "\n",
        "    # Step 3 - Concatenare una colonna di uno alla matrice delle features\n",
        "    X_train_const = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "    X_val_const   = np.c_[np.ones((X_val.shape[0], 1)), X_val]\n",
        "\n",
        "    # Step 4 - Applicare formula della regressione lineare e calcolare predizioni\n",
        "    theta = np.linalg.inv(X_train_const.T @ X_train_const) @ (X_train_const.T @ y_train)\n",
        "    y_train_pred = X_train_const @ theta\n",
        "    y_val_pred   = X_val_const @ theta\n",
        "\n",
        "    # Step 5 - Calcolare MAE\n",
        "    mae_train = np.mean(np.abs(y_train - y_train_pred))\n",
        "    mae_val   = np.mean(np.abs(y_val - y_val_pred))\n",
        "\n",
        "    return mae_train, mae_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-31T10:24:08.062037Z",
          "iopub.status.idle": "2025-03-31T10:24:08.062415Z",
          "shell.execute_reply": "2025-03-31T10:24:08.062248Z"
        },
        "trusted": true,
        "id": "tKG17O1CA5Am"
      },
      "outputs": [],
      "source": [
        "hyperparams = {'data_standardize': True, 'use_pca': True}\n",
        "train_fraction = 0.8\n",
        "validation_fraction = 0.2\n",
        "\n",
        "num_train = int(train_fraction * X.shape[0])\n",
        "\n",
        "X_train = X[:num_train]\n",
        "y_train = y[:num_train]\n",
        "\n",
        "X_validation = X[num_train:]\n",
        "y_validation = y[num_train:]\n",
        "\n",
        "# Chiamare la funzione pipeline e stampare i risultati della MAE al variare dell' utilizzo della PCA.\n",
        "\n",
        "# svolgimento...\n",
        "# Caso 1: con PCA\n",
        "hyperparams_pca = {'data_standardize': True, 'use_pca': True, 'pca_components': 10}  # specifica il numero di componenti se desiderato\n",
        "mae_train_pca, mae_val_pca = pipeline(X_train, y_train, X_validation, y_validation, hyperparams_pca)\n",
        "print(\"MAE con PCA: Training =\", mae_train_pca, \"Validation =\", mae_val_pca)\n",
        "\n",
        "# Caso 2: senza PCA\n",
        "hyperparams_no_pca = {'data_standardize': True, 'use_pca': False}\n",
        "mae_train_no_pca, mae_val_no_pca = pipeline(X_train, y_train, X_validation, y_validation, hyperparams_no_pca)\n",
        "print(\"MAE senza PCA: Training =\", mae_train_no_pca, \"Validation =\", mae_val_no_pca)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}